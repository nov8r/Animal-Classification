{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This project originated as a final project for my 'CSC 351: Machine Learning' class. I worked on this project with one of my classmates, Braydon Johnson. The goal was to make a convolutional neural network that could accurately classify different animal classes when given an image of an animal.\n",
        "\n",
        "There wasn't a whole lot of inspiration behind this so much as just a fascination with CNNs. We previously worked with CNNs in a lab for facial recognition, so we thought doing another CNN project would be fun. We also decided to include three pretrained models (ResNet50, EfficientNetB0, and MobileNetV2) as a benchmark to compare our model to.\n",
        "\n",
        "While our model is a CNN and not a residual network, like the other three, our CNN model is most similar to a residual network since a residual network is just a CNN, but with residual blocks added to it."
      ],
      "metadata": {
        "id": "kIZT3_l-gdfU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laAbooY5gP46"
      },
      "outputs": [],
      "source": [
        "# Libraries\n",
        "import kagglehub\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image as PILImage\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.models as models\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "5VNJQIUChfXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### About the data\n",
        "\n",
        "The data we used in this project is the Animals-10 dataset from kaggle.\n",
        "\n",
        "Animals-10 Dataset Details:\n",
        "- Contains ~26k Images\n",
        "- 10 Different classes:\n",
        "  - Dog, Horse, Elephant, Butterfly, Chicken, Cat, Cow, Sheep, Squirrel, and Spider\n",
        "- All images in the dataset were gathered from Google Images.\n",
        "- Dataset Link: https://www.kaggle.com/datasets/alessiocorrado99/animals10"
      ],
      "metadata": {
        "id": "r5_ouEa8g32x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data initialization\n",
        "\n",
        "# Translation for classes\n",
        "translate = {\n",
        "    \"cane\": \"dog\", \"cavallo\": \"horse\", \"elefante\": \"elephant\", \"farfalla\": \"butterfly\",\n",
        "    \"gallina\": \"chicken\", \"gatto\": \"cat\", \"mucca\": \"cow\", \"pecora\": \"sheep\",\n",
        "    \"scoiattolo\": \"squirrel\", \"ragno\": \"spider\"\n",
        "}\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "8jskMd-mg8Wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data and assign classes\n",
        "\n",
        "# Download data\n",
        "path = kagglehub.dataset_download(\"alessiocorrado99/animals10\")\n",
        "image_dir = os.path.join(path, \"raw-img\")\n",
        "\n",
        "# Assign correct classes\n",
        "translated_images = []\n",
        "class_counts = {}\n",
        "\n",
        "for italian_class in os.listdir(image_dir):\n",
        "    class_path = os.path.join(image_dir, italian_class)\n",
        "    if os.path.isdir(class_path):\n",
        "        english_class = translate.get(italian_class, italian_class)\n",
        "        image_files = [\n",
        "            os.path.join(class_path, f)\n",
        "            for f in os.listdir(class_path)\n",
        "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "        ]\n",
        "        class_counts[english_class] = len(image_files)\n",
        "        for img_path in image_files:\n",
        "            translated_images.append((img_path, english_class))"
      ],
      "metadata": {
        "id": "EviupXcqhDjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset stats\n",
        "print(f\"Total images: {len(translated_images)}\")\n",
        "for class_name, count in sorted(class_counts.items()):\n",
        "    print(f\"  {class_name}: {count} images\")"
      ],
      "metadata": {
        "id": "TsTtDOJKhKjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Transformations\n",
        "\n",
        "We came up with two different transformation sets, `train_transform` and `val_transform`. The `train_transform` set applies minor transformations to the training images to help mitigate overfiting in our model. The `val_transform` set does not apply many changes because this is where we want the model to see the true images and measure its true performance.\n",
        "\n",
        "`train_transform` changes:\n",
        "- Resize to a slightly larger image\n",
        "- Random Crop\n",
        "- Random Horizontal Flip\n",
        "- Random Rotation\n",
        "- Color Jitter\n",
        "- Image normalization*\n",
        "\n",
        "`val_transform` changes:\n",
        "- Resize to the original size\n",
        "- Convert to tensor\n",
        "- Image normalization*\n",
        "\n",
        "*normalization applied for ImageNet standards in regards to the resnet models used"
      ],
      "metadata": {
        "id": "doLeSBC1hRAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image transformations\n",
        "\n",
        "# Define separate transforms for training and validation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "_g7mjHYjhSEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Dataset\n",
        "We define a custom `AnimalDataset` class to handle our images and labels. This allows us to flexibly load images from any list (rather than a fixed folder structure) and apply transformations for data augmentation or normalization."
      ],
      "metadata": {
        "id": "j-a6jKqHhnC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset class\n",
        "class AnimalDataset(Dataset):\n",
        "    def __init__(self, image_label_list, transform=None, class_to_idx=None):\n",
        "        self.image_label_list = image_label_list\n",
        "        self.transform = transform\n",
        "        self.class_to_idx = class_to_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_label_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.image_label_list[idx]\n",
        "        image = PILImage.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "          image = self.transform(image)\n",
        "          label_idx = self.class_to_idx[label]\n",
        "        return image, label_idx"
      ],
      "metadata": {
        "id": "nKXUvdbNhoRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class names and mappings\n",
        "class_names = sorted(set(label for _, label in translated_images))\n",
        "class_to_idx = {label: idx for idx, label in enumerate(class_names)}\n",
        "idx_to_class = {idx: label for label, idx in class_to_idx.items()}\n",
        "num_classes = len(class_names)\n",
        "print(f\"Number of classes: {num_classes}\")"
      ],
      "metadata": {
        "id": "wLr6b0bsht2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train, Test, Split\n",
        "\n",
        "We split the dataset into 80% training, 10% validation, and 10% testing.  \n",
        "This ensures the model is trained on one portion, validated on unseen data during training, and evaluated on completely untouched data afterward for an honest performance measure."
      ],
      "metadata": {
        "id": "6nEngBLJhyud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train, Test, Split\n",
        "random.shuffle(translated_images)\n",
        "\n",
        "# Split up by class\n",
        "images_by_class = {}\n",
        "for img_path, label in translated_images:\n",
        "    if label not in images_by_class:\n",
        "        images_by_class[label] = []\n",
        "    images_by_class[label].append((img_path, label))\n",
        "\n",
        "train_images = []\n",
        "val_images = []\n",
        "test_images = []\n",
        "\n",
        "# 80% train, 10% validation, 10% test\n",
        "for label, images in images_by_class.items():\n",
        "    n_train = int(0.8 * len(images))\n",
        "    n_val = int(0.10 * len(images))\n",
        "\n",
        "    train_images.extend(images[:n_train])\n",
        "    val_images.extend(images[n_train:n_train+n_val])\n",
        "    test_images.extend(images[n_train+n_val:])\n",
        "\n",
        "print(f\"Train set: {len(train_images)} images\")\n",
        "print(f\"Validation set: {len(val_images)} images\")\n",
        "print(f\"Test set: {len(test_images)} images\")"
      ],
      "metadata": {
        "id": "0jl6u90thzWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Establish a dataset for Training, Validation, and Testing\n",
        "train_dataset = AnimalDataset(train_images, transform=train_transform, class_to_idx=class_to_idx)\n",
        "val_dataset = AnimalDataset(val_images, transform=val_transform, class_to_idx=class_to_idx)\n",
        "test_dataset = AnimalDataset(test_images, transform=val_transform, class_to_idx=class_to_idx)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)"
      ],
      "metadata": {
        "id": "3pZC-fShh_u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluation"
      ],
      "metadata": {
        "id": "370Rtnn3iRgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training process\n",
        "\n",
        "This function handles the full model training: running multiple epochs, optimizing the model, tracking loss and accuracy, saving the best model, and adjusting the learning rate with a scheduler.  \n",
        "We also record history for visualizing training progress later."
      ],
      "metadata": {
        "id": "0pnwRlGsiUAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Process\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10, model_name=\"Model\"):\n",
        "  model.to(device)\n",
        "  best_val_acc = 0.0\n",
        "  best_model_wts = model.state_dict()\n",
        "  history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    # tqdm settings\n",
        "    loading_bar = tqdm(train_loader, desc=f\"{model_name} Training Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for inputs, labels in loading_bar:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        num_samples += inputs.size(0)\n",
        "\n",
        "        loading_bar.set_postfix({'Loss': running_loss / num_samples, 'Accuracy': (running_corrects.double() / num_samples).item()})\n",
        "\n",
        "    epoch_loss = running_loss / num_samples\n",
        "    epoch_acc = running_corrects.double() / num_samples\n",
        "    history['train_loss'].append(epoch_loss)\n",
        "    history['train_acc'].append(epoch_acc.item())\n",
        "\n",
        "    print(f'Train Loss: {epoch_loss:.4f} Accuracy: {epoch_acc:.4f}')\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        loading_bar = tqdm(val_loader, desc=f\"{model_name} Validation Epoch {epoch+1}/{num_epochs}\")\n",
        "        for inputs, labels in loading_bar:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            loading_bar.set_postfix({'Loss': running_loss / len(val_loader.dataset), 'Accuracy': (val_corrects.double() / len(val_loader.dataset)).item()})\n",
        "\n",
        "    val_loss = val_loss / len(val_loader.dataset)\n",
        "    val_acc = val_corrects.double() / len(val_loader.dataset)\n",
        "\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc.item())\n",
        "\n",
        "    print(f'Validation Loss: {val_loss:.4f} Accuracy: {val_acc:.4f}')\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_wts = model.state_dict().copy()\n",
        "        print(f'Best model saved with accuracy: {best_val_acc:.4f}')\n",
        "        torch.save(best_model_wts, 'best_model.pth')\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step(epoch_loss)\n",
        "\n",
        "  print(f'Best Validation Accuracy: {best_val_acc:.4f}')\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model, history"
      ],
      "metadata": {
        "id": "kpL_9BeliUuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "\n",
        "After training, we evaluate the model on the test set.  \n",
        "We compute the classification report (precision, recall, F1-score), confusion matrix, and collect predictions vs. ground-truth labels for later visualization."
      ],
      "metadata": {
        "id": "iIe84VO-igBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation process\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating...\"):\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "          outputs = model(inputs)\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "          all_predictions.extend(preds.cpu().numpy())\n",
        "          all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    report = classification_report(all_labels, all_predictions, target_names = [idx_to_class[i] for i in range(num_classes)], output_dict=True)\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    return report, cm, all_predictions, all_labels"
      ],
      "metadata": {
        "id": "MnyLBJAiigfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "def plot_training(history):\n",
        "  plt.figure(figsize=(12, 4))\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(history['train_loss'], label='Training Loss')\n",
        "  plt.plot(history['val_loss'], label='Validation Loss')\n",
        "  plt.title('Loss through Epochs')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(history['train_acc'], label='Training Accuracy')\n",
        "  plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "  plt.title('Accuracy through Epochs')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "aaBSPtcLilSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix generation\n",
        "def plot_cm(cm, class_names):\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('True')\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "rKhlCEpkitbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize predictions\n",
        "def visualize_preds(model, test_dataset, num_samples=16):\n",
        "    model.eval()\n",
        "    indices = random.sample(range(len(test_dataset)), num_samples)\n",
        "    fig, axs = plt.subplots(4, 4, figsize=(12, 12))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, idx in enumerate(indices):\n",
        "            image, label = test_dataset[idx]\n",
        "            input_tensor = image.unsqueeze(0).to(device)\n",
        "            output = model(input_tensor)\n",
        "            _, pred = torch.max(output, 1)\n",
        "\n",
        "            image = image.cpu().numpy().transpose((1, 2, 0))\n",
        "\n",
        "            # Denormalize image\n",
        "            mean = np.array([0.485, 0.456, 0.406])\n",
        "            std = np.array([0.229, 0.224, 0.225])\n",
        "            image = std * image + mean\n",
        "            image = np.clip(image, 0, 1)\n",
        "\n",
        "            # Plot image\n",
        "            ax = axs.flat[i]\n",
        "            ax.imshow(image)\n",
        "            true_label = idx_to_class[label]\n",
        "            pred_label = idx_to_class[pred.item()]\n",
        "            ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\")\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "zAGNmFXVix0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output incorrect predictions\n",
        "def visualize_incorrect_preds(model, test_dataset, num_samples=16):\n",
        "  model.eval()\n",
        "  incorrect_samples = []\n",
        "  fig, axs = plt.subplots(4, 4, figsize=(12, 12))\n",
        "\n",
        "  # Randomly select incorrect predictions\n",
        "  with torch.no_grad():\n",
        "    for idx in range(len(test_dataset)):\n",
        "      image, label = test_dataset[idx]\n",
        "      input_tensor = image.unsqueeze(0).to(device)\n",
        "      output = model(input_tensor)\n",
        "      _, pred = torch.max(output, 1)\n",
        "      if pred.item() != label:\n",
        "        incorrect_samples.append((image, label, pred.item()))\n",
        "  if len(incorrect_samples) == 0:\n",
        "    print(\"No incorrect predictions found.\")\n",
        "    return\n",
        "\n",
        "  display_samples = random.sample(incorrect_samples, min(num_samples, len(incorrect_samples)))\n",
        "  for i, (image, label, pred) in enumerate(display_samples):\n",
        "    image = image.cpu().numpy().transpose((1, 2, 0))\n",
        "\n",
        "    # Denormalize image for better quality\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image = std * image + mean\n",
        "    image = np.clip(image, 0, 1)\n",
        "\n",
        "    ax = axs.flat[i]\n",
        "    ax.imshow(image)\n",
        "\n",
        "    true_label = idx_to_class[label]\n",
        "    pred_label = idx_to_class[pred]\n",
        "    ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\")\n",
        "    ax.axis('off')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig('incorrect_predictions.png')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "E2-1S5_Q-iVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolved Images\n",
        "\n",
        "To better understand what the model is learning, we visualize the feature maps produced after the first convolutional layer. This gives insights into how the model detects basic patterns like edges, textures, and shapes at early stages.\n",
        "\n",
        "Currently this is performed only on the first layer and we did it just to see the patterns it was detecting, but I think it would be better to implement this for multiple layers to get an even better understanding of how it changes throughout different epochs."
      ],
      "metadata": {
        "id": "8KvdCI1EoL6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display convolved images and their feature maps\n",
        "def convolved_images(model, model_name):\n",
        "    model.eval()\n",
        "\n",
        "    # Gather all images by class\n",
        "    class_to_images = {}\n",
        "    for img, label in test_dataset:\n",
        "        if label not in class_to_images:\n",
        "            class_to_images[label] = []\n",
        "        class_to_images[label].append(img)\n",
        "\n",
        "    # Randomly select one image per class\n",
        "    class_examples = {label: random.choice(images) for label, images in class_to_images.items()}\n",
        "\n",
        "    # Reduced figure size for compactness\n",
        "    fig, axes = plt.subplots(num_classes, 6, figsize=(15, 2.5 * num_classes))\n",
        "    fig.suptitle(\"Original Image + Convolved Outputs per Class\", fontsize=16)\n",
        "\n",
        "    for row_idx, (true_label, img) in enumerate(class_examples.items()):\n",
        "        img_input = img.unsqueeze(0).to(device)\n",
        "\n",
        "        # Predict the class\n",
        "        with torch.no_grad():\n",
        "            output = model(img_input)\n",
        "            _, predicted_label = torch.max(output, 1)\n",
        "            predicted_class_name = idx_to_class[predicted_label.item()]\n",
        "            true_class_name = idx_to_class[true_label]\n",
        "\n",
        "        # Pass through only the early layers\n",
        "        with torch.no_grad():\n",
        "            if model_name == \"Custom\":\n",
        "                features = model.conv1(img_input)\n",
        "            elif model_name == \"ResNet50\":\n",
        "                features = list(model.children())[0](img_input)\n",
        "            elif model_name == \"MobileNetV2\":\n",
        "                features = model.features[0](img_input)\n",
        "            elif model_name == \"EfficientNetB0\":\n",
        "                features = model.features[0][0](img_input)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported model_name: {model_name}\")\n",
        "\n",
        "        features = features.squeeze(0).cpu()\n",
        "\n",
        "        # Return image back to original for plotting\n",
        "        image = img.cpu().numpy().transpose((1, 2, 0))\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "\n",
        "        # Original image\n",
        "        ax = axes[row_idx, 0]\n",
        "        ax.imshow(image)\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f\"True: {true_class_name}\\nPred: {predicted_class_name}\", fontsize=8)\n",
        "\n",
        "        # Feature maps\n",
        "        for i in range(5):\n",
        "            ax = axes[row_idx, i+1]\n",
        "            ax.imshow(features[i], cmap='viridis')\n",
        "            ax.axis('off')\n",
        "            if row_idx == 0:\n",
        "                ax.set_title(f'Feature Map {i+1}', fontsize=7)\n",
        "\n",
        "    # Tighter layout with reduced padding\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.97], pad=0.5)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "LMlSEMVE-vy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define models\n",
        "\n",
        "# Custom Model\n",
        "class CustomModel(nn.Module):\n",
        "  def __init__(self, num_classes, dropout_rate=0.5):\n",
        "    super(CustomModel, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "    self.bn3 = nn.BatchNorm2d(128)\n",
        "    self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "    self.bn4 = nn.BatchNorm2d(256)\n",
        "    self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.feature_size = self._get_feature_size(224)\n",
        "\n",
        "    self.fc1 = nn.Linear(self.feature_size, 512)\n",
        "    self.bn5 = nn.BatchNorm1d(512)\n",
        "    self.dropout1 = nn.Dropout(dropout_rate)\n",
        "    self.fc2 = nn.Linear(512, 256)\n",
        "    self.bn6 = nn.BatchNorm1d(256)\n",
        "    self.dropout2 = nn.Dropout(dropout_rate)\n",
        "    self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "  def _get_feature_size(self, input_size):\n",
        "    size = input_size // 2\n",
        "    size = size // 2\n",
        "    size = size // 2\n",
        "    size = size // 2\n",
        "    return 256 * size * size\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool1(F.leaky_relu(self.bn1(self.conv1(x)), 0.1))\n",
        "    x = self.pool2(F.leaky_relu(self.bn2(self.conv2(x)), 0.1))\n",
        "    x = self.pool3(F.leaky_relu(self.bn3(self.conv3(x)), 0.1))\n",
        "    x = self.pool4(F.leaky_relu(self.bn4(self.conv4(x)), 0.1))\n",
        "\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    x = F.leaky_relu(self.bn5(self.fc1(x)), 0.1)\n",
        "    x = self.dropout1(x)\n",
        "    x = F.leaky_relu(self.bn6(self.fc2(x)), 0.1)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "# Resnet Model\n",
        "model_res = models.resnet50(weights='DEFAULT')\n",
        "num_ftrs = model_res.fc.in_features\n",
        "model_res.fc = nn.Linear(num_ftrs, num_classes)\n",
        "model_res.to(device)\n",
        "\n",
        "# Efficient Model\n",
        "model_enet = models.efficientnet_b0(weights='DEFAULT')\n",
        "num_ftrs = model_enet.classifier[1].in_features\n",
        "model_enet.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
        "model_enet.to(device)\n",
        "\n",
        "# Mobile Model\n",
        "model_mnet = models.mobilenet_v2(weights='DEFAULT')\n",
        "num_ftrs = model_mnet.classifier[1].in_features\n",
        "model_mnet.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
        "model_mnet.to(device)"
      ],
      "metadata": {
        "id": "EO-0RHnf-1TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Training, Testing, and Evaluation\n",
        "\n",
        "We train each model (Custom CNN, ResNet50, EfficientNetB0, MobileNetV2), evaluate their performance, visualize predictions and misclassifications, extract feature maps, and save the trained weights for future use."
      ],
      "metadata": {
        "id": "y3onM1GcpZ1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Training, testing, and evaluation of the code\n",
        "models = [\n",
        "    (\"Custom\", CustomModel(num_classes)),\n",
        "    (\"ResNet50\", model_res),\n",
        "    (\"EfficientNetB0\", model_enet),\n",
        "    (\"MobileNetV2\", model_mnet)\n",
        "]\n",
        "\n",
        "for model_name, model in models:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "  # Train model\n",
        "  num_epochs = 10\n",
        "  trained_model, history = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, model_name)\n",
        "  plot_training(history)\n",
        "\n",
        "  print(\"\\nEvaluating on test set...\")\n",
        "  report, cm, all_predictions, all_labels = evaluate_model(trained_model, test_loader)\n",
        "\n",
        "  # Classification report\n",
        "  print(\"\\nClassification Report:\")\n",
        "  for class_name, metrics in report.items():\n",
        "    if class_name in ['accuracy']:\n",
        "      continue\n",
        "    print(f\"{class_name}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1-score']:.4f}\")\n",
        "  print(f\"\\nOverall Accuracy: {report['accuracy']:.4f}\")\n",
        "\n",
        "  # Confusion Matrix\n",
        "  print(\"\\nConfusion Matrix:\")\n",
        "  plot_cm(cm, [idx_to_class[i] for i in range(num_classes)])\n",
        "\n",
        "  # Convolved Images\n",
        "  convolved_images(trained_model, model_name)\n",
        "\n",
        "  # Visualize predictions\n",
        "  visualize_preds(trained_model, test_dataset)\n",
        "  visualize_incorrect_preds(trained_model, test_dataset)\n",
        "\n",
        "  # Save model\n",
        "  torch.save({\n",
        "        'model_state_dict': trained_model.state_dict(),\n",
        "        'class_to_idx': class_to_idx,\n",
        "        'idx_to_class': idx_to_class,\n",
        "        'model_name': model\n",
        "  }, f'final_animal_classifier{model_name}.pth')\n",
        "\n",
        "print(trained_model, class_to_idx, idx_to_class)"
      ],
      "metadata": {
        "id": "yjbKgj2V-5hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Thoughts\n",
        "\n",
        "This project taught us a lot about CNNs and Residual Networks; We learned a lot throughout the whole process from the creation of our models to the evaluation.\n",
        "\n",
        "Ultimately, our custom CNN implementation was too simple and underperformed significantly compared to the pretrained residual networks. It might be a good idea to go back and revisit our model and figure out ways to improve it.\n",
        "\n",
        "We also tried to make our own custom implementation of a residual network, but it didn't workout well and we were short on time, but it may also be worth it to go back and revist that as well.\n",
        "\n",
        "End Model Results:\n",
        "\n",
        "| Model          | Accuracy | Precision | f1-Score | Recall  |\n",
        "| :-----         | :------: | :-------: | :------: | :-----: |\n",
        "| Custom         | 0.7568   | 0.7714    | 0.7292   | 0.7150  |\n",
        "| ResNet50       | 0.9151   | 0.9043    | 0.9080   | 0.9152  |\n",
        "| EfficientNetB2 | 0.9410   | 0.9413    | 0.9370   | 0.9339  |\n",
        "| MobileNetV2    | 0.9242   | 0.9268    | 0.9170   | 0.9177  |"
      ],
      "metadata": {
        "id": "iwJ1Xz2UpipR"
      }
    }
  ]
}